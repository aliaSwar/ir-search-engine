{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ir_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mir_datasets\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ir_datasets'"
     ]
    }
   ],
   "source": [
    "\n",
    "import ir_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ir_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mir_datasets\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m dataset \u001b[39m=\u001b[39m ir_datasets\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mantique/train/split200-train\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ir_datasets'"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "import pandas as pd\n",
    "\n",
    "dataset = ir_datasets.load(\"antique/train/split200-train\")\n",
    "df=pd.DataFrame(dataset.docs)\n",
    "df.head(10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# remove stop words _1\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))  \n",
    "df['text_to'] = df['text'].apply(lambda x: \" \".join([w for w in str(x).split() if not w in stop_words]))\n",
    "df.head(10)\n",
    "\n",
    "#remove punctuation_2\n",
    "df['text_to'] = df['text_to'].str.replace('[^\\w\\s]','')\n",
    "df['text_to']\n",
    "\n",
    "\n",
    "#caselower_3\n",
    "df['text_to'] = df['text_to'].str.lower()\n",
    "\n",
    "\n",
    "# toknize _4\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "# nltk.download('book')\n",
    "import re\n",
    "df['text_to']=df['text_to'].apply(lambda x: word_tokenize(x))\n",
    "df.head(10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lemtization _5\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "df['text_to'] = df['text_to'].apply(lemmatize_text)\n",
    "df['text_to'].head(10)\n",
    "\n",
    "\n",
    "\n",
    "#stemaization_6\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# Use English stemmer.\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "df['text_to'] = df['text_to'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tf idf _\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def dummy(tokens):\n",
    "    return tokens\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=dummy, preprocessor=dummy)\n",
    "X = vectorizer.fit_transform(df['text_to'][: 15000].values)\n",
    "df1 = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "#print(df1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#inverted index _\n",
    "from collections import defaultdict\n",
    "\n",
    "inv_indx = defaultdict(list)\n",
    "for term in df1.columns:\n",
    "    indices= df1[df1[term] > 0].index.tolist()\n",
    "    inv_indx[term]=indices\n",
    "\n",
    "\n",
    "for term , indices in inv_indx.items():   \n",
    "    print(f\"{term}: {indices}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
